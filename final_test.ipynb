{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "import nltk\n",
    "#nltk.download('punkt')  # Download the necessary tokenizer data (only required once)\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"BERT.pdf\"\n",
    "test_query_raw = \"What is the key reference of the paper?\" # reference related\n",
    "#test_query_raw = \"What performance did BERT achieve on natural language processing tasks?\" #direct query\n",
    "#test_query_raw = \"what is the key insight of the proposed method?\" #indirect query\n",
    "raw_text = functions.extract_raw_text_from_pdf(file_path)\n",
    "in_text_citations, cleaned_text = functions.extract_in_text_citations(raw_text)#cleaned_text is changed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_citation_context = pd.DataFrame(columns=['citation', 'context'])\n",
    "for citation in in_text_citations:\n",
    "    context = functions.get_context(raw_text, citation)\n",
    "    new_row = pd.DataFrame({'citation': [citation], 'context': [context]})\n",
    "    raw_citation_context = pd.concat([raw_citation_context, new_row], ignore_index=True)\n",
    "\n",
    "#raw_citation_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract Refernces part\n",
    "references = re.findall(r'References.*', cleaned_text) \n",
    "# check if there is \"Appendix\" part after references\n",
    "appendix = re.findall(r'Appendix.*', cleaned_text)\n",
    "# find the word \"References\" and remove everything after it\n",
    "cleaned_text0 = re.sub(r'References.*', '', cleaned_text) + ' '.join(appendix)\n",
    "# add some key words to the beginning of the text\n",
    "cleaned_text = \"title \" + \"author \" + cleaned_text0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = cleaned_text.split(' ')\n",
    "# split the text into chunks of 350 words with 50 words overlap\n",
    "chunks = [' '.join(all_sentences[i:i+300]) for i in range(0, len(all_sentences), 250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [title, author, bert, pretraining, deep, bidir...\n",
       "1     [task, featurebased, andﬁnetuning, featurebase...\n",
       "2     [lefttoright, language, model, pretraining, ml...\n",
       "3     [objective, used, well, objective, discriminat...\n",
       "4     [supervised, downstream, task, advantage, appr...\n",
       "5     [transfer, learning, large, pretrained, model,...\n",
       "6     [total, parameters110m, bert, large, l24, h102...\n",
       "7     [figure, 1, denote, input, embedding, e, ﬁnal,...\n",
       "8     [literature, case, ﬁnal, hidden, vector, corre...\n",
       "9     [sentence, aandbfor, pretraining, example, 50,...\n",
       "10    [critical, use, documentlevel, corpus, rather,...\n",
       "11    [gpu, starting, exact, pretrained, model7we, d...\n",
       "12    [httpsgluebenchmarkcomleaderboard, number, tas...\n",
       "13    [obtains, 728, date, writing, ﬁnd, bert, large...\n",
       "14    [batch, size, 32, table, 2, show, top, leaderb...\n",
       "15    [742, 771, published, unet, ensemble, 714, 749...\n",
       "16    [dev, set, maximize, f1, use, triviaqa, data, ...\n",
       "17    [849, 865, 926, 879, ltr, nsp, 821, 843, 775, ...\n",
       "18    [nsp, ltr, nsp, ltr, model, performs, worse, m...\n",
       "19    [report, average, dev, set, accuracy, 5, rando...\n",
       "20    [improvement, prior, work, used, featurebased,...\n",
       "21    [12, 468, 819, 848, 913, 12, 768, 12, 399, 844...\n",
       "22    [03, f1, behind, ﬁnetuning, entire, model, dem...\n",
       "23    [nspcls, helikesplay, ingsep, mydogiscutesepin...\n",
       "24    [us, selfattention, mechanism, unify, two, sta...\n",
       "25    [hidden, vectorc2rhcorresponding, ﬁrst, input,...\n",
       "26    [selected, best, ﬁnetuning, learning, rate, am...\n",
       "27    [10httpsgluebenchmarkcomleaderboardwikipedia, ...\n",
       "28    [top, ensemble, system, term, f1, score, witho...\n",
       "29    [squad, v11, bert, model, task, treat, questio...\n",
       "30    [task, choose, plausible, continuation, among,...\n",
       "31    [importance, deep, bidirectionality, bert, eva...\n",
       "32    [performance, glue, task, recognize, would, al...\n",
       "33    [encoder, largest, transformer, found, literat...\n",
       "34    [downstream, task, however, featurebased, appr...\n",
       "35    [hidden, 956, last, hidden, 949, weighted, sum...\n",
       "36    [ollgraf, 2018, contextual, string, embeddings...\n",
       "37    [modeling, arxiv, preprint, arxiv13123005, z, ...\n",
       "38    [abs160608415, felix, hill, kyunghyun, cho, an...\n",
       "39    [dagan, 2016, context2vec, learning, generic, ...\n",
       "40    [2016, conference, empirical, method, natural,...\n",
       "41    [2018a, glue, multitask, benchmark, analysis, ...\n",
       "42    [urtasun, antonio, torralba, sanja, fidler, 20...\n",
       "43    [know, word, asked, predict, replaced, random,...\n",
       "44    [empirical, improvement, mlm, model, far, outw...\n",
       "45    [use, gelu, activation, rather, standard, relu...\n",
       "46    [development, set, a4, comparison, bert, elmo,...\n",
       "47    [taskspeciﬁc, ﬁnetuning, learning, rate, perfo...\n",
       "48    [semantically, equivalent, qnli, question, nat...\n",
       "49    [collection, sentence, pair, drawn, news, head...\n",
       "50    [accuracy, mnli, trained, 1m, step, compared, ...\n",
       "51    [843, 949, 940, 80, 0, 20, 841, 952, 946, 80, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe with the chunks\n",
    "raw_df = pd.DataFrame(chunks, columns=['text'])\n",
    "# process the sentences\n",
    "df = raw_df['text'].apply(functions.process_sentence)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_test_query = functions.process_sentence(test_query_raw)\n",
    "# remove \"paper\",\"text\",\"article\",\"thesis\" from the query\n",
    "in_test_query = [x for x in in_test_query if x not in [\"paper\",\"text\",\"article\",\"thesis\"]]\n",
    "\n",
    "selected_rows = []\n",
    "\n",
    "# if the query contains 'reference','references' or 'citation' ,'citations'\n",
    "if any(x in in_test_query for x in ['reference','references','citation','citations']):\n",
    "    df_chosen_temp = raw_citation_context\n",
    "    # set the citation as the index of the dataframe\n",
    "    df_chosen_temp = df_chosen_temp.set_index('citation')\n",
    "    # df_chosen should be a dataframe series\n",
    "    df_chosen = df_chosen_temp['context']\n",
    "    # process the sentences\n",
    "    df_chosen = df_chosen.apply(functions.process_sentence)\n",
    "    test_query = in_test_query\n",
    "\n",
    "else:\n",
    "    # find all the rows containing at least one word in the query\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if any(x in df[i] for x in in_test_query):\n",
    "            selected_rows.append(i)\n",
    "\n",
    "    # check if the query if direct in the text or not\n",
    "    if len(selected_rows) >= 1:\n",
    "        # extract the selected_rows from the dataframe with thier index and create a new dataframe\n",
    "        df_selected = df.iloc[selected_rows]\n",
    "        df_chosen = df_selected \n",
    "        test_query = in_test_query\n",
    "    else:\n",
    "        df_chosen = df\n",
    "         # expand query in some case\n",
    "        expand_query = in_test_query.copy()\n",
    "        for word in in_test_query:\n",
    "            synonyms = functions.find_synonyms(word)\n",
    "            try:\n",
    "                expand_query.append(synonyms.pop())\n",
    "            except KeyError:\n",
    "                continue\n",
    "        test_query = expand_query   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(selected_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained glove word embeddings\n",
    "embeddings_dict = {}\n",
    "with open(\"glove/glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_dict[word] = vector\n",
    "\n",
    "# create a vocabulary\n",
    "vocabulary = set()\n",
    "for sentence in df:\n",
    "    for w in sentence:\n",
    "        vocabulary.add(w)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "corpus = []\n",
    "for sentence in df:\n",
    "    for word in sentence:\n",
    "        corpus.append(word)\n",
    "\n",
    "# create a dictionary to store the unigram probability of each word\n",
    "unigram_probabilities = {}\n",
    "for word in vocabulary:\n",
    "    unigram_probabilities[word] = functions.calculate_unigram_probability(corpus, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dict = df_chosen.to_dict() # set in correct input format\n",
    "df_dict_vec = functions.sentence_embedding(embeddings_dict, df_dict, 0.5, unigram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_dict = {0: test_query}\n",
    "test_query_vec = functions.sentence_embedding(embeddings_dict, test_query_dict, 0.5, unigram_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 3 similar chunks\n",
    "def get_top3_similar_chunks(query):\n",
    "    ranking = {}\n",
    "    for q in df_dict_vec:\n",
    "        ranking[q] = functions.cosine_similarity(query, df_dict_vec[q])\n",
    "    ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "    if len(ranking) < 2:\n",
    "        return ranking\n",
    "    return ranking[:2]\n",
    "\n",
    "# get the top 5 similar chunks\n",
    "def get_top5_similar_chunks(query):\n",
    "    ranking = {}\n",
    "    for q in df_dict_vec:\n",
    "        ranking[q] = functions.cosine_similarity(query, df_dict_vec[q])\n",
    "    ranking = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "    if len(ranking) < 5:\n",
    "        return ranking\n",
    "    return ranking[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "citation\n",
       "(Peters et al., 2018a; Radford et al., 2018)    [unlike, recent, language, representation, mod...\n",
       "(Bowman et al., 2015; Williams et al., 2018)    [include, sentencelevel, task, natural, langua...\n",
       "(Peters et al., 2018a)                          [featurebased, approach, elmo, peter, et, al, ...\n",
       "(Radford et al., 2018)                          [ﬁnetuning, approach, generative, pretrained, ...\n",
       "(Vaswani et al., 2017)                          [example, openai, gpt, author, use, lefttorigh...\n",
       "                                                                      ...                        \n",
       "(Warstadt et al., 2018)                         [cola, corpus, linguistic, acceptability, bina...\n",
       "(Cer et al., 2017)                              [stsb, semantic, textual, similarity, benchmar...\n",
       "(Dolan and Brockett, 2005)                      [2018, paraphrasing, dolan, brockett, 2005, ai...\n",
       "(Bentivogli et al., 2009)                       [rte, recognizing, textual, entailment, binary...\n",
       "(Levesque et al., 2011)                         [14, wnli, winograd, nli, small, natural, lang...\n",
       "Name: context, Length: 93, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Peters et al., 2018a; Radford et al., 2018)</td>\n",
       "      <td>Unlike recent language representation models ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Bowman et al., 2015; Williams et al., 2018)</td>\n",
       "      <td>These include sentence-level tasks such as na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Peters et al., 2018a)</td>\n",
       "      <td>The feature-based approach, such as ELMo  (Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Radford et al., 2018)</td>\n",
       "      <td>The ﬁne-tuning approach, such as the Generati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Vaswani et al., 2017)</td>\n",
       "      <td>For example, in OpenAI GPT, the authors use a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>(Warstadt et al., 2018)</td>\n",
       "      <td>CoLA The Corpus of Linguistic Acceptability i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>(Cer et al., 2017)</td>\n",
       "      <td>STS-B The Semantic Textual Similarity Benchma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>(Dolan and Brockett, 2005)</td>\n",
       "      <td>, 2018) and paraphrasing  (Dolan and Brockett,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>(Bentivogli et al., 2009)</td>\n",
       "      <td>RTE Recognizing Textual Entailment is a binar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>(Levesque et al., 2011)</td>\n",
       "      <td>14 WNLI Winograd NLI is a small natural langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        citation  \\\n",
       "0   (Peters et al., 2018a; Radford et al., 2018)   \n",
       "1   (Bowman et al., 2015; Williams et al., 2018)   \n",
       "2                         (Peters et al., 2018a)   \n",
       "3                         (Radford et al., 2018)   \n",
       "4                         (Vaswani et al., 2017)   \n",
       "..                                           ...   \n",
       "88                       (Warstadt et al., 2018)   \n",
       "89                            (Cer et al., 2017)   \n",
       "90                    (Dolan and Brockett, 2005)   \n",
       "91                     (Bentivogli et al., 2009)   \n",
       "92                       (Levesque et al., 2011)   \n",
       "\n",
       "                                              context  \n",
       "0    Unlike recent language representation models ...  \n",
       "1    These include sentence-level tasks such as na...  \n",
       "2    The feature-based approach, such as ELMo  (Pe...  \n",
       "3    The ﬁne-tuning approach, such as the Generati...  \n",
       "4    For example, in OpenAI GPT, the authors use a...  \n",
       "..                                                ...  \n",
       "88   CoLA The Corpus of Linguistic Acceptability i...  \n",
       "89   STS-B The Semantic Textual Similarity Benchma...  \n",
       "90  , 2018) and paraphrasing  (Dolan and Brockett,...  \n",
       "91   RTE Recognizing Textual Entailment is a binar...  \n",
       "92  14 WNLI Winograd NLI is a small natural langua...  \n",
       "\n",
       "[93 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_citation_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(Peters et al., 2018a; Radford et al., 2018)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test5 = get_top5_similar_chunks(test_query_vec[0])\n",
    "test5[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(Peters et al., 2018a; Radford et al., 2018): Unlike recent language representation models  (Peters et al., 2018a; Radford et al., 2018) , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers;(Bowman et al., 2015; Williams et al., 2018): These include sentence-level tasks such as natural language inference  (Bowman et al., 2015; Williams et al., 2018)  and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce ﬁne-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al;(Peters et al., 2018a): The feature-based approach, such as ELMo  (Peters et al., 2018a) , uses task-speciﬁc architectures that include the pre-trained representations as additional features;(Radford et al., 2018): The ﬁne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT)  (Radford et al., 2018) , introduces minimal task-speciﬁc parameters, and is trained on the downstream tasks by simply ﬁne-tuning allpretrained parameters;(Vaswani et al., 2017): For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer  (Vaswani et al., 2017) ;'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_index_of_value(df, value):\n",
    "    try:\n",
    "        index = df[df == value].stack().index[0]\n",
    "        return index\n",
    "    except IndexError:\n",
    "        return None\n",
    "    \n",
    "# for loop to get the top 5 index of the citation and concatenate all the results\n",
    "result = \"\"\n",
    "for i in range(5):\n",
    "    index, column = get_index_of_value(raw_citation_context, test5[i][0])\n",
    "    result += test5[i][0] + \":\"+ raw_citation_context.iloc[index]['context'] + \";\"\n",
    "\n",
    "# # Example usage\n",
    "# index, column = get_index_of_value(raw_citation_context, test5[0][0])\n",
    "# # use the index get the context\n",
    "# result = \" \" + test5[0][0] + \":\"+ raw_citation_context.iloc[index]['context'] + \"\\n\"\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melindadong/Desktop/A_game/NLP/NLP_Final/functions.py:86: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = dot_product / (norm_a * norm_b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The key reference of the paper is Peters et al., 2018a.\n"
     ]
    }
   ],
   "source": [
    "# if the query contains 'reference','references' or 'citation' ,'citations'\n",
    "if any(x in in_test_query for x in ['reference','references','citation','citations']):\n",
    "\n",
    "    # get top 5 similar chunks\n",
    "    top_5_dict = get_top5_similar_chunks(test_query_vec[0])\n",
    "    top_5_dict\n",
    "\n",
    "    # concanate all the key values of the top 5 chunks\n",
    "    top_5_chunks = []\n",
    "    for i in range(len(top_5_dict)):\n",
    "        top_5_chunks.append(top_5_dict[i][0])\n",
    "\n",
    "    #remove \"()\" in the in_text_citations\n",
    "    in_text_citations0 = [re.sub(r'[()]', '', x) for x in top_5_chunks]\n",
    "    # extract the citations contains \";\" and split them into two citations\n",
    "    in_text_citations1 = [x.split('; ') for x in in_text_citations0]\n",
    "    # flatten the list\n",
    "    in_text_citations2 = [item for sublist in in_text_citations1 for item in sublist]\n",
    "    # create a doctionary to store the citations, the key is the citation and the value is the counts\n",
    "    in_text_citations_dict = {}\n",
    "    for i in in_text_citations2:\n",
    "        if i in in_text_citations_dict:\n",
    "            in_text_citations_dict[i] += 1\n",
    "        else:\n",
    "            in_text_citations_dict[i] = 1\n",
    "\n",
    "    # order the dictionary by the counts\n",
    "    in_text_citations_dict = dict(sorted(in_text_citations_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    #print(top_5_chunks)\n",
    "\n",
    "\n",
    "    #print(\"Answer: The top 5 important references are: \\n (listed in order) \\n\", list(in_text_citations_dict.keys())[:5])\n",
    "    def ask_question(paragraph, question):\n",
    "        chat_history = [\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': 'Here are some texts from a paper, the format is \"in-text citations: the content of that citation;\"' + paragraph},\n",
    "            {'role': 'assistant', 'content': question}\n",
    "        ]\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=chat_history\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        return answer\n",
    "\n",
    "    paragraph = result\n",
    "    #question = \"A reference might be used in a paper for different purposes. For example, some are used as representative works in a research direction, some are used as the reference for a dataset. What we are looking for here is a paper that directly influence the design of the research method in the current paper.\" + test_query_raw\n",
    "    question = test_query_raw\n",
    "\n",
    "\n",
    "    answer = ask_question(paragraph, question)\n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "\n",
    "else:\n",
    "    top_3_dict = get_top3_similar_chunks(test_query_vec[0])\n",
    "\n",
    "    # the the index from the dictionary keys\n",
    "    top_3_index = [int(i[0]) for i in top_3_dict]\n",
    "    final_chunk = ''\n",
    "    for i in top_3_index:\n",
    "        final_chunk += raw_df.iloc[i][\"text\"]\n",
    "\n",
    "\n",
    "    def ask_question(paragraph, question):\n",
    "        chat_history = [\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "            {'role': 'user', 'content': 'Here are some texts from a paper: ' + paragraph},\n",
    "            {'role': 'assistant', 'content': question}\n",
    "        ]\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=chat_history\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        return answer\n",
    "\n",
    "    paragraph = final_chunk\n",
    "    question = test_query_raw\n",
    "\n",
    "\n",
    "\n",
    "    answer = ask_question(paragraph, question)\n",
    "    print(\"Answer:\", answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
